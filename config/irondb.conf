<?xml version="1.0" encoding="utf8"?>
<snowth lockfile="/irondb/logs/snowth.lock" text_size_limit="512">
  <eventer>
    <config>
      <ssl_dhparam1024_file/>
      <ssl_dhparam2048_file/>
    </config>
  </eventer>

  <include file="circonus-watchdog.conf" snippet="true"/>
  <include file="{{pkg.svc_config_path}}/licenses.conf" snippet="true"/>

  <!--
    LRU cache of open filehandles for numeric metric rollups
      Can improve read latency for frequently-accessed metrics.

      Size is a licensed parameter and may not be larger than your
      license allows (cache_size). If this parameter does not appear
      in your license file, the default below may not be changed.

      If increased, the IRONdb process may need to have its ulimit
      for open files raised.
  -->
  <cache cpubuckets="128" size="0"/>

  <logs>
    <log name="default" type="file" path="/irondb/logs/errorlog" timestamps="on" rotate_seconds="86400" retain_seconds="604800"/>
    <log name="http/access" type="file" path="/irondb/logs/accesslog" rotate_seconds="3600" retain_seconds="604800"/>
    <log name="notice/startup" type="file" path="/irondb/logs/startuplog" timestamps="on" rotate_seconds="86400" retain_seconds="604800"/>
    <log name="internal" type="memory" path="50000,1000000"/>
    <errors>
      <outlet name="default"/>
      <outlet name="stderr"/>
      <outlet name="internal"/>
      <log name="error" disabled="false"/>
      <log name="debug" disabled="true"/>
      <log name="debug/eventer" disabled="true"/>
      <log name="debug/http" disabled="true"/>
      <log name="http/io" disabled="true"/>
    </errors>
  </logs>

  <listeners>
    <!--
      Main listener. Used for numerous functions, including:
        * Metric submission via HTTP REST API
        * Intra-cluster communication (gossip, replication)
        * Admin UI
        * Node performance stats (/stats.json)
    -->
    <listener address="{{sys.ip}}" port="8112" backlog="100" type="http_rest_api">
      <config>
        <document_root>/opt/circonus/share/snowth-web</document_root>
      </config>
    </listener>

    <!--
      Management CLI for application internals.
    -->
    <listener address="127.0.0.1" port="32322" type="mtev_console">
      <config>
        <line_protocol>telnet</line_protocol>
      </config>
    </listener>

    <!--
      Graphite listener
        This installs a network socket graphite listener under the account
        specified by <account_id>, named <check_name>.
        Metrics will appear as "graphite.<check_name>.a.b.c.d".

        To get metrics into this "check" over HTTP instead, you would POST to:

        http://thismachine:8112/graphite/1/ebb0d8e4-35c2-636d-a994-f46e22949f4f/placeholder_check
    -->
    <listener address="*" port="2003" type="graphite">
      <config>
        <check_uuid>{{cfg.default_check_uuid}}</check_uuid>
        <check_name>{{cfg.default_check_name}}</check_name>
        <account_id>1</account_id>
      </config>
    </listener>

    <!--
      Pickle listener
        This installs a network socket pickle listener under the account
        specified by <account_id>, named <check_name>.

        Off by default. Uncomment the stanza below and restart to activate.
    -->
    <!--
    <listener address="*" port="2004" type="graphite_pickle">
      <config>
        <check_uuid>ebb0d8e4-35c2-636d-a994-f46e22949f4f</check_uuid>
        <check_name>placeholder_check</check_name>
        <account_id>1</account_id>
      </config>
    </listener>
    -->
  </listeners>

  <rest>
    <acl>
      <rule type="allow" />
    </acl>
  </rest>

  <pools>
    <rollup concurrency="1"/>
    <nnt_put concurrency="4"/>
    <raw_writer concurrency="4"/>
    <rest_graphite_numeric_get concurrency="4"/>
    <rest_graphite_find_metrics concurrency="4"/>
    <rest_graphite_fetch_metrics concurrency="10"/>
  </pools>

  <!-- Text and Histogram metric types are not used in IRONdb -->
  <text directory="/irondb/text/{node}"/>
  <histogram directory="/irondb/hist/{node}">
    {{~#each cfg.rollups as |rollup|}}
    <rollup period="{{rollup.period}}"/>
    {{~/each}}
  </histogram>

  <!--
    Numeric metric rollups
      The periods defined here need to match the periods defined under <nntbs>
  -->
  <rollups>
    {{~#each cfg.rollups as |rollup|}}
    <rollup period="{{rollup.period}}" directory="/irondb/data/{node}/{{rollup.directory}}"//>
    {{~/each}}
  </rollups>
  <!--
    NNTBS
      The periods defined here need to match the periods defined under <rollups>

      NOTE: this is sized for an installation where each node holds 100K unique metric
            names.  If you plan on a larger installation, you should revise
            downwards the size of each <shard> below.  Contact Circonus Support
            for sizing recommendations.
  -->
  <nntbs path="/irondb/nntbs/{node}">
    {{~#each cfg.rollups as |rollup|}}
    <shard period="{{rollup.period}}" size="{{rollup.nntbs_shard_size}}"//>
    {{~/each}}
  </nntbs>
  <localstate directory="/irondb/localstate"/>

  <!-- Helper databases -->
  <metric_name_database location="/irondb/metric_name_db/{node}"/>
  <surrogate_database location="/irondb/surrogate_db/{node}" db_type="rocksdb" />

  <!--
    Raw metric database
      This configures the treatment of the raw data that IRONdb ingests.

      These settings will:
        * shard your data into 1-week buckets (granularity)
        * roll up and delete a bucket after it is 4 weeks old (min_delete_age)
          and has not been written to in the past 2 hours (delete_after_quiescent_age).
        * allow the submission of metrics timestamped up to 1 day in the future,
          to accommodate clients with incorrect clocks (max_clock_skew).
        * Use the most efficient rollup strategy when converting raw data
          to its rolled up representation

      DO NOT change granularity after beginning to store data,
      as this may result in data loss.

      The granularity, min_delete_age, delete_after_quiescent_age, and max_clock_skew
      attributes are defined as second-resolution mtev durations, please see:
      http://circonus-labs.github.io/libmtev/apireference/c.html#mtevconfstrparseduration
      http://circonus-labs.github.io/libmtev/apireference/c.html#mtevgetdurationss
  -->
  <raw_database location="/irondb/raw_db/{node}"
                data_db="{{cfg.raw_database.data_db}}"
                granularity="{{cfg.raw_database.granularity}}"
                min_delete_age="{{cfg.raw_database.min_delete_age}}"
                delete_after_quiescent_age="{{cfg.raw_database.delete_after_quiescent_age}}"
                max_clock_skew="{{cfg.raw_database.max_clock_skew}}"
		            rollup_strategy="{{cfg.raw_database.rollup_strategy}}"
  />

  <!--
    Journal settings
      Journals are write-ahead logs for replicating metric data to other nodes.
      Each node has one journal for each of its cluster peers.

      These settings will:
        * establish 4 concurrent threads for writing to each peer journal,
          improving ingestion throughput (concurrency).
        * cause outbound journal messages to be sent in batches of up to
          25K messages (max_bundled_messages).
        * use an in-memory buffer of 128KB for new journal messages,
          which will be flushed to the log file when full (pre_commit_size).

      Note that a concurrency of 4 is enough to provide up to 700K metrics/second
      ingestion throughput, and isn't likely to need tuning except in the most
      extreme cases.
  -->
  <journal concurrency="4"
           max_bundled_messages="25000"
           pre_commit_size="131072"
  />

  <!-- Cluster definition -->
  <include file="/opt/circonus/etc/topology_snippet.conf" snippet="true"/>
  <topology path="/opt/circonus/etc/irondb-topo"
            active="b1b9e15a4624d19923246fae16094473976a7db18800abd564e8b088824c2507"
            next=""
            redo="/irondb/redo/{node}"
  />
</snowth>
